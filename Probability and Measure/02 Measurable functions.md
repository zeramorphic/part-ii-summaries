- A function $f \colon E \to G$ (between measurable spaces) is measurable if the inverse image of a measurable set is measurable.
- $\mathbb 1_A$ is a measurable function iff $A$ is a measurable set.
- Measurability of a function is preserved under sum, product, countable inf/sup, countable liminf/limsup.

> [!theorem]
> **Monotone class theorem**. Let $\mathcal A$ be a $\pi$-system that generates a given $\sigma$-algebra on $E$. Let $\mathcal V$ be a set of bounded maps $E \to \mathbb R$. Suppose that
> - $\mathcal V$ is a real vector space;
> - $\mathcal V$ contains indicator functions for all $A \in \mathcal A$ and $E$ itself;
> - $\mathcal V$ is closed under countable sup of nonnegative pointwise increasing functions (as long as the sup is bounded).
> 
> Then $\mathcal V$ has all bounded measurable functions $E \to \mathbb R$.

Proof uses Dynkin. This is a tool that lets us prove results about basic inputs and generalise to lots of inputs. For example, take $\mathcal A = \{(-\infty,a] \mid a \in \mathbb R\}$, which generates $\mathcal B$.

- Let $f \colon E \to G$. The image measure $\nu$ on $G$ is $\nu(A) = \mu(f^{-1}(A))$.
- Let $g \colon \mathbb R \to \mathbb R$ be an increasing, right-continuous function, then there exists a unique Radon measure $\mu_g$ on $\mathbb R$ such that $\mu_g((a,b]) = g(b) - g(a)$. All Radon measures are obtained in this way. Think: $g$ is a cumulative distribution function $g(x) = \mathbb P(X \leq x)$. Such measures are called Lebesgue-Stieltjes measures; $g$ is the Stieltjes distribution.

Random variables.
- An $E$-valued random variable is a measurable $X \colon \Omega \to E$. A random variable without any other description is taken to mean $\Omega \to \mathbb R$. A random vector is $\Omega \to \mathbb R^d$.
- The law or distribution of an r.v. is the image measure $\mu_X = \mathbb P \circ X^{-1}$ on $E$. If $E = \mathbb R$, $\mu_X$ has a unique distribution function $F_X(z) = \mathbb P(X \leq z)$.
- Random variables $X_i$ are independent if the $\sigma$-algebras generated by inverse images of measurable sets $X_i^{-1}(A)$ are independent. Think: for an r.v. $X$, the $\sigma$-algebra $\sigma(\{X^{-1}(A)\})$ indicates the only ways that $X$ can depend on the outcome $\omega$. Equivalently on $\mathbb R$, finite joint probabilities ($\mathbb P(X_1 \leq x_1, \dots, X_n \leq x_n)$) factorise as products of probabilities.

Convergence. For probability terminology, use the names marked $\mathbb P$.
- $f_n \to f$ almost everywhere ($\mathbb P$: almost surely) if the set on which $f_n(x) \not\to f(x)$ is null.
- $f_n \to f$ in measure ($\mathbb P$: in probability; $X_n \to^p X$) if for all $\varepsilon > 0$, the set of $x$ for which $|f_n(x) - f(x)| > \varepsilon$ has measure converging to 0.
- Let $X_n, X$ be r.vs. $X_n \to X$ in distribution ($X_n \to^d X$) if $\mathbb P(X_n \leq x) \to \mathbb P(X \leq x)$ at all points at which the limit is continuous.
The notions of convergence can be related in a few ways.
- If $\mu(E) < \infty$, $f_n \to 0$ a.e. implies $f_n \to 0$ in measure. (Not if $\mu(E) = \infty$, take $f_n = \mathbb 1_{(n,\infty)}$)
- $f_n\to 0$ in measure implies $f_{n_k} \to 0$ a.e. on some subsequence. (Not if we omit "on some subsequence", take independent events $A_n$ with $\mathbb P(A_n) = \frac{1}{n}$, use Borel-Cantelli)
- Convergence in probability implies convergence in distribution.

> [!theorem]
> **Kolmogorov's zero-one law**. Let $X_n$ be independent r.vs, and let $A$ be an event in the tail $\sigma$-algebra. Then $\mathbb P(A)$ is either 0 or 1. If $Y \colon \Omega \to \mathbb R$ is tail-$\sigma$-algebra-measurable, it is constant almost surely.

Proof idea: $A$ is independent from itself.